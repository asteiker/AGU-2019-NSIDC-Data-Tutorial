{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discover, Customize and Access NSIDC DAAC Data\n",
    "\n",
    "This notebook is based off of the NSIDC-Data-Access-Notebook provided here: \n",
    "https://github.com/nsidc/NSIDC-Data-Access-Notebook\n",
    "\n",
    "Now that we've visualized our study areas, we will explore data coverage, size, and customization service availability. We will then access the data we're interested in utilizing the NSIDC DAAC's Data Access and Service API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, getpass, socket, json, zipfile, io, math, os, shutil, pprint, re, time\n",
    "from statistics import mean\n",
    "from requests.auth import HTTPBasicAuth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Earthdata Login credentials\n",
    "\n",
    "An Earthdata Login account is required to access data from the NSIDC DAAC. If you do not already have an Earthdata Login account, visit http://urs.earthdata.nasa.gov to register."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Earthdata Login user name:  amy.steiker\n",
      "Earthdata Login password:  ·········\n",
      "Email address associated with Earthdata Login account:  amy.steiker@nsidc.org\n"
     ]
    }
   ],
   "source": [
    "# Earthdata Login credentials\n",
    "\n",
    "uid = input('Earthdata Login user name: ')\n",
    "pswd = getpass.getpass('Earthdata Login password: ')\n",
    "email = input('Email address associated with Earthdata Login account: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select data sets and determine version numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bt': {'short_name': 'AU_SI6'},\n",
      " 'ist': {'short_name': 'MOD29'},\n",
      " 'photon_height': {'short_name': 'ATL03'},\n",
      " 'sea_ice_height': {'short_name': 'ATL07'}}\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary of data set parameters we'll use in our access API command below. We'll start with data set IDs (e.g. ATL07) of interest here, also known as \"short name\".\n",
    "\n",
    "data_dict = {\n",
    "    'photon_height' : {'short_name' : 'ATL03'},\n",
    "    'sea_ice_height' : {'short_name' : 'ATL07'},\n",
    "    'ist' : {'short_name' : 'MOD29'},\n",
    "    'bt' : {'short_name' : 'AU_SI6'}\n",
    "}\n",
    "pprint.pprint(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most recent version of  ATL03  is  002\n",
      "The most recent version of  ATL07  is  002\n",
      "The most recent version of  MOD29  is  6\n",
      "The most recent version of  AU_SI6  is  1\n"
     ]
    }
   ],
   "source": [
    "# Get json response from CMR collection metadata\n",
    "\n",
    "for i in range(len(data_dict)):\n",
    "    cmr_collections_url = 'https://cmr.earthdata.nasa.gov/search/collections.json'\n",
    "    response = requests.get(cmr_collections_url, params=list(data_dict.values())[i])\n",
    "    results = json.loads(response.content) \n",
    "\n",
    "    # Find all instances of 'version_id' in metadata and print most recent version number\n",
    "    versions = [el['version_id'] for el in results['feed']['entry']]\n",
    "    versions = [i for i in versions if not any(c.isalpha() for c in i)]\n",
    "    data_dict[list(data_dict.keys())[i]]['version'] = max(versions)\n",
    "    print('The most recent version of ', list(data_dict.values())[i]['short_name'], ' is ', max(versions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select time and area of interest\n",
    "\n",
    "Data granules are returned based on a spatial bounding box and temporal range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change following bbox and time code blocks - hardcode these into dictionary itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140,72,153,80\n"
     ]
    }
   ],
   "source": [
    "# Bounding Box spatial parameter in 'W,S,E,N' format\n",
    "\n",
    "# Input lower left longitude in decimal degrees\n",
    "LL_lon = '140'\n",
    "# Input lower left latitude in decimal degrees\n",
    "LL_lat = '72'\n",
    "# Input upper right longitude in decimal degrees\n",
    "UR_lon = '153'\n",
    "# Input upper right latitude in decimal degrees\n",
    "UR_lat = '80'\n",
    "\n",
    "bounding_box = LL_lon + ',' + LL_lat + ',' + UR_lon + ',' + UR_lat\n",
    "\n",
    "#add bounding_box to each data set dictionary\n",
    "for k, v in data_dict.items(): data_dict[k]['bounding_box'] = bounding_box\n",
    "\n",
    "print(bounding_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-23T00:00:00Z,2019-03-23T23:59:59Z\n"
     ]
    }
   ],
   "source": [
    "#Input temporal range \n",
    "\n",
    "# Input start date in yyyy-MM-dd format\n",
    "start_date = '2019-03-23'\n",
    "# Input start time in HH:mm:ss format\n",
    "start_time = '00:00:00'\n",
    "# Input end date in yyyy-MM-dd format\n",
    "end_date = '2019-03-23'\n",
    "# Input end time in HH:mm:ss format\n",
    "end_time = '23:59:59'\n",
    "\n",
    "temporal = start_date + 'T' + start_time + 'Z' + ',' + end_date + 'T' + end_time + 'Z'\n",
    "\n",
    "#add temporal to each data set dictionary\n",
    "for k, v in data_dict.items(): data_dict[k]['temporal'] = temporal\n",
    "\n",
    "print(temporal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine how many granules exist over this time and area of interest, as well as the average size and total volume of those granules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 granules of ATL03 version 002 over my area and time of interest.\n",
      "The average size of each granule is 1685.59 MB and the total size of all 4 granules is 6742.36 MB\n",
      "\n",
      "There are 3 granules of ATL07 version 002 over my area and time of interest.\n",
      "The average size of each granule is 260.65 MB and the total size of all 3 granules is 781.94 MB\n",
      "\n",
      "There are 13 granules of MOD29 version 6 over my area and time of interest.\n",
      "The average size of each granule is 2.75 MB and the total size of all 13 granules is 35.70 MB\n",
      "\n",
      "There are 2 granules of AU_SI6 version 1 over my area and time of interest.\n",
      "The average size of each granule is 88.35 MB and the total size of all 2 granules is 176.69 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query number of granules (paging over results)\n",
    "granule_search_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n",
    "for i in range(len(data_dict)):\n",
    "    params = {\n",
    "        'short_name': list(data_dict.values())[i]['short_name'],\n",
    "        'version': list(data_dict.values())[i]['version'],\n",
    "        'bounding_box': bounding_box,\n",
    "        'temporal': temporal,\n",
    "        'page_size': 100,\n",
    "        'page_num': 1\n",
    "    }\n",
    "    granules = []\n",
    "    headers={'Accept': 'application/json'}\n",
    "    while True:\n",
    "        response = requests.get(granule_search_url, params=params, headers=headers)\n",
    "        results = json.loads(response.content)\n",
    "\n",
    "        if len(results['feed']['entry']) == 0:\n",
    "            # Out of results, so break out of loop\n",
    "            break\n",
    "\n",
    "        # Collect results and increment page_num\n",
    "        granules.extend(results['feed']['entry'])\n",
    "        params['page_num'] += 1\n",
    "    print('There are', len(granules), 'granules of', list(data_dict.values())[i]['short_name'], 'version', list(data_dict.values())[i]['version'], 'over my area and time of interest.')\n",
    "    for k, v in data_dict.items(): data_dict[k]['gran_num'] = len(granules)\n",
    "    granule_sizes = [float(granule['granule_size']) for granule in granules]\n",
    "    print(f'The average size of each granule is {mean(granule_sizes):.2f} MB and the total size of all {len(granules)} granules is {sum(granule_sizes):.2f} MB')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that subsetting, reformatting, or reprojecting can alter the size of the granules if those services are applied to your request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the subsetting, reformatting, and reprojection services enabled for your data set of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NSIDC DAAC supports customization services on many of our NASA Earthdata mission collections. Let's discover whether or not our data set has these services available. If services are available, we will also determine the specific service options supported for this data set and select which of these services we want to request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATL03 service selection:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Subsetting by bounding box, based on the area of interest inputted above, is available. Would you like to request this service? (y/n) y\n",
      "Subsetting by time, based on the temporal range inputted above, is available. Would you like to request this service? (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These reformatting options are available: ['TABULAR_ASCII', 'NetCDF4-CF', 'NetCDF-3']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "If you would like to reformat, copy and paste the reformatting option you would like (make sure to omit quotes, e.g. GeoTIFF), otherwise leave blank. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No reprojection options are supported with your requested format\n",
      "\n",
      "ATL07 service selection:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Subsetting by bounding box, based on the area of interest inputted above, is available. Would you like to request this service? (y/n) y\n",
      "Subsetting by time, based on the temporal range inputted above, is available. Would you like to request this service? (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These reformatting options are available: ['TABULAR_ASCII', 'NetCDF4-CF', 'NetCDF-3']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "If you would like to reformat, copy and paste the reformatting option you would like (make sure to omit quotes, e.g. GeoTIFF), otherwise leave blank. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No reprojection options are supported with your requested format\n",
      "\n",
      "MOD29 service selection:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Subsetting by bounding box, based on the area of interest inputted above, is available. Would you like to request this service? (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These reformatting options are available: ['GeoTIFF']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "If you would like to reformat, copy and paste the reformatting option you would like (make sure to omit quotes, e.g. GeoTIFF), otherwise leave blank. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These reprojection options are available with your requested format: ['GEOGRAPHIC', 'UNIVERSAL TRANSVERSE MERCATOR', 'POLAR STEREOGRAPHIC', 'SINUSOIDAL']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "If you would like to reproject, copy and paste the reprojection option you would like (make sure to omit quotes, e.g. GEOGRAPHIC), otherwise leave blank. POLAR STEREOGRAPHIC\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AU_SI6 service selection:\n",
      "No services exist for AU_SI6\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Would like to receive XML metadata files along with the science files? (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "for k, v in data_dict.items():\n",
    "    sn = v['short_name']\n",
    "    ve = (v['version'])\n",
    "    capability_url = f'https://n5eil02u.ecs.nsidc.org/egi/capabilities/{sn}.{ve}.xml'\n",
    "    \n",
    "    # Create session to store cookie and pass credentials to capabilities url\n",
    "    session = requests.session()\n",
    "    s = session.get(capability_url)\n",
    "    response = session.get(s.url,auth=(uid,pswd))\n",
    "    root = ET.fromstring(response.content)\n",
    "\n",
    "    #collect lists with each service option\n",
    "    subagent = [subset_agent.attrib for subset_agent in root.iter('SubsetAgent')]\n",
    "\n",
    "    # variable subsetting\n",
    "    variables = [SubsetVariable.attrib for SubsetVariable in root.iter('SubsetVariable')]  \n",
    "    variables_raw = [variables[i]['value'] for i in range(len(variables))]\n",
    "    variables_join = [''.join(('/',v)) if v.startswith('/') == False else v for v in variables_raw] \n",
    "    variable_vals = [v.replace(':', '/') for v in variables_join]\n",
    "\n",
    "    # reformatting\n",
    "    formats = [Format.attrib for Format in root.iter('Format')]\n",
    "    format_vals = [formats[i]['value'] for i in range(len(formats))]\n",
    "    if format_vals : format_vals.remove('')\n",
    "\n",
    "    # reprojection options\n",
    "    projections = [Projection.attrib for Projection in root.iter('Projection')]\n",
    "    proj_vals = []\n",
    "    for i in range(len(projections)):\n",
    "        if (projections[i]['value']) != 'NO_CHANGE' :\n",
    "            proj_vals.append(projections[i]['value'])\n",
    "\n",
    "    #print service information depending on service availability and select service options\n",
    "    print(sn, 'service selection:')\n",
    "    if len(subagent) < 1 :\n",
    "            print('No services exist for', sn)\n",
    "            meta = input('Would like to receive XML metadata files along with the science files? (y/n)')\n",
    "            if meta == 'n': data_dict[k]['INCLUDE_META'] = 'N'\n",
    "            print('')\n",
    "    else:\n",
    "        subdict = subagent[0]\n",
    "        if subdict['spatialSubsetting'] == 'true':\n",
    "            ss = input('Subsetting by bounding box, based on the area of interest inputted above, is available. Would you like to request this service? (y/n)')\n",
    "            if ss == 'y': data_dict[k]['bbox'] = bounding_box\n",
    "        if subdict['temporalSubsetting'] == 'true':\n",
    "            ts = input('Subsetting by time, based on the temporal range inputted above, is available. Would you like to request this service? (y/n)')\n",
    "            if ts == 'y': data_dict[k]['time'] = start_date + 'T' + start_time + ',' + end_date + 'T' + end_time \n",
    "        if len(format_vals) > 0 :\n",
    "            print('These reformatting options are available:', format_vals)\n",
    "            reformat = input('If you would like to reformat, copy and paste the reformatting option you would like (make sure to omit quotes, e.g. GeoTIFF), otherwise leave blank.')\n",
    "        if len(proj_vals) > 0 : \n",
    "            print('These reprojection options are available with your requested format:', proj_vals)\n",
    "            data_dict[k]['projection'] = input('If you would like to reproject, copy and paste the reprojection option you would like (make sure to omit quotes, e.g. GEOGRAPHIC), otherwise leave blank.')\n",
    "            # Enter required parameters for UTM North and South\n",
    "            if data_dict[k]['projection'] == 'UTM NORTHERN HEMISPHERE' or data_dict[k]['projection'] == 'UTM SOUTHERN HEMISPHERE': \n",
    "                data_dict[k]['NZone'] = input('Please enter a UTM zone (1 to 60 for Northern Hemisphere; -60 to -1 for Southern Hemisphere):')\n",
    "                data_dict[k]['projection_parameters'] = str('NZone:' + NZone)\n",
    "        else: \n",
    "            print('No reprojection options are supported with your requested format')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's select a subset of variables. We'll use these primary variables of interest for the ICESat-2 sea ice and photon height products:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATL07\n",
    "#Use only strong beams\n",
    "\n",
    "data_dict['sea_ice_height']['coverage'] = '/gt1l/sea_ice_segments/delta_time,\\\n",
    "/gt1l/sea_ice_segments/latitude,\\\n",
    "/gt1l/sea_ice_segments/longitude,\\\n",
    "/gt1l/sea_ice_segments/heights/height_segment_confidence,\\\n",
    "/gt1l/sea_ice_segments/heights/height_segment_height,\\\n",
    "/gt1l/sea_ice_segments/heights/height_segment_quality,\\\n",
    "/gt1l/sea_ice_segments/heights/height_segment_surface_error_est,\\\n",
    "/gt2l/sea_ice_segments/delta_time,\\\n",
    "/gt2l/sea_ice_segments/latitude,\\\n",
    "/gt2l/sea_ice_segments/longitude,\\\n",
    "/gt2l/sea_ice_segments/heights/height_segment_confidence,\\\n",
    "/gt2l/sea_ice_segments/heights/height_segment_height,\\\n",
    "/gt2l/sea_ice_segments/heights/height_segment_quality,\\\n",
    "/gt2l/sea_ice_segments/heights/height_segment_surface_error_est,\\\n",
    "/gt3l/sea_ice_segments/delta_time,\\\n",
    "/gt3l/sea_ice_segments/latitude,\\\n",
    "/gt3l/sea_ice_segments/longitude,\\\n",
    "/gt3l/sea_ice_segments/heights/height_segment_confidence,\\\n",
    "/gt3l/sea_ice_segments/heights/height_segment_height,\\\n",
    "/gt3l/sea_ice_segments/heights/height_segment_quality,\\\n",
    "/gt3l/sea_ice_segments/heights/height_segment_surface_error_est'\n",
    "\n",
    "#ATL03\n",
    "#Use only strong beams\n",
    "\n",
    "data_dict['photon_height']['coverage'] = '/ds_surf_type,\\\n",
    "/gt1l/heights/delta_time,\\\n",
    "/gt1l/heights/h_ph,\\\n",
    "/gt1l/heights/lat_ph,\\\n",
    "/gt1l/heights/lon_ph,\\\n",
    "/gt1l/heights/signal_conf_ph,\\\n",
    "/gt2l/heights/delta_time,\\\n",
    "/gt2l/heights/h_ph,\\\n",
    "/gt2l/heights/lat_ph,\\\n",
    "/gt2l/heights/lon_ph,\\\n",
    "/gt2l/heights/signal_conf_ph,\\\n",
    "/gt3l/heights/delta_time,\\\n",
    "/gt3l/heights/h_ph,\\\n",
    "/gt3l/heights/lat_ph,\\\n",
    "/gt3l/heights/lon_ph,\\\n",
    "/gt3l/heights/signal_conf_ph'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select data access configurations\n",
    "\n",
    "The data request can be accessed asynchronously or synchronously. The asynchronous option will allow concurrent requests to be queued and processed without the need for a continuous connection. Those requested orders will be delivered to the specified email address, or they can be accessed programmatically as shown below. Synchronous requests will automatically download the data as soon as processing is complete. For this tutorial, we will be selecting the asynchronous method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There will be 1 total order(s) processed for our ATL03 request.\n",
      "There will be 1 total order(s) processed for our ATL07 request.\n",
      "There will be 1 total order(s) processed for our MOD29 request.\n",
      "There will be 1 total order(s) processed for our AU_SI6 request.\n"
     ]
    }
   ],
   "source": [
    "#Set NSIDC data access base URL\n",
    "base_url = 'https://n5eil02u.ecs.nsidc.org/egi/request'\n",
    "\n",
    "for k, v in data_dict.items():\n",
    "    #Add email address\n",
    "    data_dict[k]['email'] = email\n",
    "    \n",
    "    #Set the request mode to asynchronous\n",
    "    data_dict[k]['request_mode'] = 'async'\n",
    "\n",
    "    #Set the page size to the maximum for asynchronous requests \n",
    "    page_size = 2000\n",
    "    data_dict[k]['page_size'] = page_size\n",
    "\n",
    "    #Determine number of orders needed for requests over 2000 granules. \n",
    "    page_num = math.ceil(data_dict[k]['gran_num']/page_size)\n",
    "    data_dict[k]['page_num'] = page_num\n",
    "    del data_dict[k]['gran_num']\n",
    "    print('There will be', page_num, 'total order(s) processed for our', v['short_name'], 'request.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the API endpoint \n",
    "\n",
    "Programmatic API requests are formatted as HTTPS URLs that contain key-value-pairs specifying the service operations that we specified above. The following command can be executed via command line, a web browser, or in Python below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "https://n5eil02u.ecs.nsidc.org/egi/request?short_name=ATL03&version=002&bounding_box=140,72,153,80&temporal=2019-03-23T00:00:00Z,2019-03-23T23:59:59Z&bbox=140,72,153,80&time=2019-03-23T00:00:00,2019-03-23T23:59:59&coverage=/ds_surf_type,/gt1l/heights/delta_time,/gt1l/heights/h_ph,/gt1l/heights/lat_ph,/gt1l/heights/lon_ph,/gt1l/heights/signal_conf_ph,/gt2l/heights/delta_time,/gt2l/heights/h_ph,/gt2l/heights/lat_ph,/gt2l/heights/lon_ph,/gt2l/heights/signal_conf_ph,/gt3l/heights/delta_time,/gt3l/heights/h_ph,/gt3l/heights/lat_ph,/gt3l/heights/lon_ph,/gt3l/heights/signal_conf_ph&email=amy.steiker@nsidc.org&request_mode=async&page_size=2000&page_num=1\n",
      "\n",
      "https://n5eil02u.ecs.nsidc.org/egi/request?short_name=ATL07&version=002&bounding_box=140,72,153,80&temporal=2019-03-23T00:00:00Z,2019-03-23T23:59:59Z&bbox=140,72,153,80&time=2019-03-23T00:00:00,2019-03-23T23:59:59&coverage=/gt1l/sea_ice_segments/delta_time,/gt1l/sea_ice_segments/latitude,/gt1l/sea_ice_segments/longitude,/gt1l/sea_ice_segments/heights/height_segment_confidence,/gt1l/sea_ice_segments/heights/height_segment_height,/gt1l/sea_ice_segments/heights/height_segment_quality,/gt1l/sea_ice_segments/heights/height_segment_surface_error_est,/gt2l/sea_ice_segments/delta_time,/gt2l/sea_ice_segments/latitude,/gt2l/sea_ice_segments/longitude,/gt2l/sea_ice_segments/heights/height_segment_confidence,/gt2l/sea_ice_segments/heights/height_segment_height,/gt2l/sea_ice_segments/heights/height_segment_quality,/gt2l/sea_ice_segments/heights/height_segment_surface_error_est,/gt3l/sea_ice_segments/delta_time,/gt3l/sea_ice_segments/latitude,/gt3l/sea_ice_segments/longitude,/gt3l/sea_ice_segments/heights/height_segment_confidence,/gt3l/sea_ice_segments/heights/height_segment_height,/gt3l/sea_ice_segments/heights/height_segment_quality,/gt3l/sea_ice_segments/heights/height_segment_surface_error_est&email=amy.steiker@nsidc.org&request_mode=async&page_size=2000&page_num=1\n",
      "\n",
      "https://n5eil02u.ecs.nsidc.org/egi/request?short_name=MOD29&version=6&bounding_box=140,72,153,80&temporal=2019-03-23T00:00:00Z,2019-03-23T23:59:59Z&bbox=140,72,153,80&projection=POLAR STEREOGRAPHIC&email=amy.steiker@nsidc.org&request_mode=async&page_size=2000&page_num=1\n",
      "\n",
      "https://n5eil02u.ecs.nsidc.org/egi/request?short_name=AU_SI6&version=1&bounding_box=140,72,153,80&temporal=2019-03-23T00:00:00Z,2019-03-23T23:59:59Z&INCLUDE_META=N&email=amy.steiker@nsidc.org&request_mode=async&page_size=2000&page_num=1\n"
     ]
    }
   ],
   "source": [
    "endpoint_list = [] \n",
    "for k, v in data_dict.items():\n",
    "    param_string = '&'.join(\"{!s}={!r}\".format(k,v) for (k,v) in v.items())\n",
    "    param_string = param_string.replace(\"'\",\"\")\n",
    "    \n",
    "    #Print API base URL + request parameters\n",
    "    API_request = api_request = f'{base_url}?{param_string}'\n",
    "    endpoint_list.append(API_request)\n",
    "    if data_dict[k]['page_num'] > 1:\n",
    "        for i in range(data_dict[k]['page_num']):\n",
    "            page_val = i + 2\n",
    "            data_dict[k]['page_num'] = page_val\n",
    "            API_request = api_request = f'{base_url}?{param_string}'\n",
    "            endpoint_list.append(API_request)\n",
    "\n",
    "print(\"\\n\".join(\"\\n\"+s for s in endpoint_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now download data using the Python requests library. The data will be downloaded directly to this notebook directory in a new Outputs folder. The progress of each order will be reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATL03 Order:  1\n",
      "Request HTTP response:  201\n",
      "order ID:  5000000412850\n",
      "status URL:  https://n5eil02u.ecs.nsidc.org/egi/request/5000000412850\n",
      "HTTP response from order response URL:  201\n",
      "Data request  1  is submitting...\n",
      "Initial request status is  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  complete\n",
      "Zip download URL:  https://n5eil02u.ecs.nsidc.org/esir/5000000412850.zip\n",
      "Beginning download of zipped output...\n",
      "Data request 1 is complete.\n",
      "\n",
      "ATL07 Order:  1\n",
      "Request HTTP response:  201\n",
      "order ID:  5000000412880\n",
      "status URL:  https://n5eil02u.ecs.nsidc.org/egi/request/5000000412880\n",
      "HTTP response from order response URL:  201\n",
      "Data request  1  is submitting...\n",
      "Initial request status is  pending\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  complete_with_errors\n",
      "error messages:\n",
      "['166328502:NoMatchingData - No data found that matched subset constraints. '\n",
      " 'Exit code 3.',\n",
      " 'PT24.375S',\n",
      " 'ICESAT2']\n",
      "Zip download URL:  https://n5eil02u.ecs.nsidc.org/esir/5000000412880.zip\n",
      "Beginning download of zipped output...\n",
      "Data request 1 is complete.\n",
      "\n",
      "MOD29 Order:  1\n",
      "Request HTTP response:  201\n",
      "order ID:  5000000412881\n",
      "status URL:  https://n5eil02u.ecs.nsidc.org/egi/request/5000000412881\n",
      "HTTP response from order response URL:  201\n",
      "Data request  1  is submitting...\n",
      "Initial request status is  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  complete\n",
      "Zip download URL:  https://n5eil02u.ecs.nsidc.org/esir/5000000412881.zip\n",
      "Beginning download of zipped output...\n",
      "Data request 1 is complete.\n",
      "\n",
      "AU_SI6 Order:  1\n",
      "Request HTTP response:  200\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-94dd266bb5ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0morder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mesir_root\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./order/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0morderlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0morderID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morderlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'order ID: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morderID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Create an output folder if the folder does not already exist.\n",
    "path = str(os.getcwd() + '/Outputs')\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "# Request data service for each page number, and unzip outputs\n",
    "for k, v in data_dict.items():\n",
    "    for i in range(data_dict[k]['page_num']):\n",
    "        page_val = i + 1\n",
    "        print(v['short_name'], 'Order: ', page_val)\n",
    "\n",
    "    # For all requests other than spatial file upload, use get function\n",
    "        request = session.get(base_url, params=v.items())\n",
    "        print('Request HTTP response: ', request.status_code)\n",
    "\n",
    "    # Raise bad request: Loop will stop for bad response code.\n",
    "        request.raise_for_status()\n",
    "        #print('Order request URL: ', request.url)\n",
    "        esir_root = ET.fromstring(request.content)\n",
    "        #print('Order request response XML content: ', request.content)\n",
    "\n",
    "    #Look up order ID\n",
    "        orderlist = []   \n",
    "        for order in esir_root.findall(\"./order/\"):\n",
    "            orderlist.append(order.text)\n",
    "        orderID = orderlist[0]\n",
    "        print('order ID: ', orderID)\n",
    "\n",
    "    #Create status URL\n",
    "        statusURL = base_url + '/' + orderID\n",
    "        print('status URL: ', statusURL)\n",
    "\n",
    "    #Find order status\n",
    "        request_response = session.get(statusURL)    \n",
    "        print('HTTP response from order response URL: ', request_response.status_code)\n",
    "\n",
    "    # Raise bad request: Loop will stop for bad response code.\n",
    "        request_response.raise_for_status()\n",
    "        request_root = ET.fromstring(request_response.content)\n",
    "        statuslist = []\n",
    "        for status in request_root.findall(\"./requestStatus/\"):\n",
    "            statuslist.append(status.text)\n",
    "        status = statuslist[0]\n",
    "        print('Data request ', page_val, ' is submitting...')\n",
    "        print('Initial request status is ', status)\n",
    "\n",
    "    #Continue loop while request is still processing\n",
    "        while status == 'pending' or status == 'processing': \n",
    "            print('Status is not complete. Trying again.')\n",
    "            time.sleep(10)\n",
    "            loop_response = session.get(statusURL)\n",
    "\n",
    "    # Raise bad request: Loop will stop for bad response code.\n",
    "            loop_response.raise_for_status()\n",
    "            loop_root = ET.fromstring(loop_response.content)\n",
    "\n",
    "    #find status\n",
    "            statuslist = []\n",
    "            for status in loop_root.findall(\"./requestStatus/\"):\n",
    "                statuslist.append(status.text)\n",
    "            status = statuslist[0]\n",
    "            print('Retry request status is: ', status)\n",
    "            if status == 'pending' or status == 'processing':\n",
    "                continue\n",
    "\n",
    "    #Order can either complete, complete_with_errors, or fail:\n",
    "    # Provide complete_with_errors error message:\n",
    "        if status == 'complete_with_errors' or status == 'failed':\n",
    "            messagelist = []\n",
    "            for message in loop_root.findall(\"./processInfo/\"):\n",
    "                messagelist.append(message.text)\n",
    "            print('error messages:')\n",
    "            pprint.pprint(messagelist)\n",
    "\n",
    "    # Download zipped order if status is complete or complete_with_errors\n",
    "        if status == 'complete' or status == 'complete_with_errors':\n",
    "            downloadURL = 'https://n5eil02u.ecs.nsidc.org/esir/' + orderID + '.zip'\n",
    "            print('Zip download URL: ', downloadURL)\n",
    "            print('Beginning download of zipped output...')\n",
    "            zip_response = session.get(downloadURL)\n",
    "            # Raise bad request: Loop will stop for bad response code.\n",
    "            zip_response.raise_for_status()\n",
    "            with zipfile.ZipFile(io.BytesIO(zip_response.content)) as z:\n",
    "                z.extractall(path)\n",
    "            print('Data request', page_val, 'is complete.')\n",
    "        else: print('Request failed.')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\\n<eesi:agentResponse xsi:schemaLocation=\"http://eosdis.nasa.gov/esi/rsp/e https://newsroom.gsfc.nasa.gov/esi/8.1/schemas/ESIAgentResponseExternal.xsd\" xmlns=\"\" xmlns:iesi=\"http://eosdis.nasa.gov/esi/rsp/i\" xmlns:ssw=\"http://newsroom.gsfc.nasa.gov/esi/rsp/ssw\" xmlns:eesi=\"http://eosdis.nasa.gov/esi/rsp/e\" xmlns:esi=\"http://eosdis.nasa.gov/esi/rsp\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">\\n    <downloadUrls>\\n        <downloadUrl>ftp://n5eil02u.ecs.nasa.gov/DP1/AMSA/AU_SI6.001/2019.03.22/AMSR_U2_L3_SeaIce6km_B02_20190322.he5</downloadUrl>\\n        <downloadUrl>ftp://n5eil02u.ecs.nasa.gov/DP1/AMSA/AU_SI6.001/2019.03.23/AMSR_U2_L3_SeaIce6km_B02_20190323.he5</downloadUrl>\\n    </downloadUrls>\\n</eesi:agentResponse>\\n'\n"
     ]
    }
   ],
   "source": [
    "print(request.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we will clean up the Output folder by removing individual order folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up Outputs folder by removing individual granule folders \n",
    "\n",
    "for root, dirs, files in os.walk(path, topdown=False):\n",
    "    for file in files:\n",
    "        try:\n",
    "            shutil.move(os.path.join(root, file), path)\n",
    "        except OSError:\n",
    "            pass\n",
    "    for name in dirs:\n",
    "        os.rmdir(os.path.join(root, name))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To review, we have explored data availability and volume over a region and time of interest, discovered and selected data customization options, constructed API endpoints for our requests, and downloaded data. Let's move on to the analysis portion of the tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
